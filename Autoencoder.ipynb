{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T08:28:25.227286Z",
     "start_time": "2020-10-29T08:28:24.457373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\zzhuqshun\\.conda\\envs\\AppliedDL2023\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import meshio\n",
    "import os\n",
    "from scipy.interpolate import griddata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0.006, 0.0135, (0.0135-0.006)/300)\n",
    "y = np.arange(0, 0.0025, 0.0025/75)\n",
    "\n",
    "data_path = './Data'\n",
    "path_sep = '\\\\' # use '/' for Unix and '\\\\' for Windows\n",
    "folders = os.listdir(data_path)\n",
    "        \n",
    "subfolder = []\n",
    "path = []\n",
    "names = []\n",
    "condition = []\n",
    "\n",
    "for i in folders:\n",
    "    if os.path.isdir(data_path + path_sep + i): \n",
    "        subfolder.append(data_path + path_sep + i)\n",
    "\n",
    "for folder in subfolder:\n",
    "    files = os.listdir(folder + path_sep)\n",
    "    for i in files:\n",
    "        ext = os.path.splitext(i)\n",
    "        if (ext[-1].lower() == '.vtk') & (ext[0][-2] != '_'):\n",
    "            names.append(ext[0])\n",
    "            string = ext[0].replace('ER', '').replace('Tin', '').replace('Uin', '').replace('Twall', '').split('_')[0:4]\n",
    "            var = []\n",
    "            for j in string:\n",
    "                if j == 'Adiabatic':\n",
    "                    var.append(0.)\n",
    "                else:\n",
    "                    var.append(float(j))\n",
    "            condition.append(var)\n",
    "\n",
    "Q_list = []\n",
    "for folder in subfolder:\n",
    "    files = os.listdir(folder + path_sep)\n",
    "    for counter,file in enumerate(files):\n",
    "        mesh = meshio.read(folder+ path_sep +file)\n",
    "        points = mesh.points\n",
    "        Qdot = mesh.point_data['Qdot']\n",
    "        boolArr = (points[:,1] == 0) & (points[:,0] >= 0.006)  \n",
    "        Qdot = Qdot[boolArr]\n",
    "        points = points[boolArr]\n",
    "        old_points = points[:,[0, 2]]\n",
    "        grid_x, grid_y = np.meshgrid(x, y)\n",
    "        grid_new = griddata(old_points, Qdot, (grid_x, grid_y), method='nearest')\n",
    "        Q_list.append(grid_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T08:28:27.512819Z",
     "start_time": "2020-10-29T08:28:27.353250Z"
    }
   },
   "outputs": [],
   "source": [
    "Qdot = np.array(Q_list)\n",
    "Qdot = Qdot / np.max(Qdot)\n",
    "mean = Qdot.mean(axis = 0)\n",
    "Qdot = np.reshape(Qdot, (-1, 75, 300, 1))\n",
    "\n",
    "normaliser = []\n",
    "conditions = np.array(condition)\n",
    "df = np.zeros(conditions.shape)\n",
    "for i in range(conditions.shape[1]):\n",
    "    df[:,i] = conditions[:,i] / np.max(conditions[:,i])\n",
    "    normaliser.append(np.max(conditions[:,i]))\n",
    "\n",
    "train_data, test_data, label_train, label_test = train_test_split(Qdot, df, test_size = 0.15)\n",
    "print(Qdot.shape)\n",
    "print(df.shape)\n",
    "# Manual train test split if needed\n",
    "#test_index = \n",
    "\n",
    "#train_index = \n",
    "\n",
    "#test_data = Qdot[test_index]\n",
    "#train_data = Qdot[train_index]\n",
    "#label_train = df[train_index]\n",
    "#label_test = df[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 80))\n",
    "plt.imshow(np.reshape(Qdot[0],(75, 300)))\n",
    "# plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 16))\n",
    "columns = 3\n",
    "rows = 12\n",
    "\n",
    "for i in range(1, columns * rows):\n",
    "    img = Qdot[(i-1)]\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.imshow(img)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(tf.keras.Model):\n",
    "  \"\"\"Convolutional autoencoder.\"\"\"\n",
    "\n",
    "  def __init__(self, latent_dim):\n",
    "    super(CVAE, self).__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "    self.encoder = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.InputLayer(input_shape=(75, 300, 1)),\n",
    "            # 75x300x1\n",
    "            tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=(1, 1), padding='same',\n",
    "                                   activation='relu', name='conv_1'),\n",
    "            tf.keras.layers.BatchNormalization(name='bn_1'),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(1, 2), name='maxpool_1')(x),\n",
    "            # 75x150x32\n",
    "            \n",
    "            tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=(1, 1), padding='same',\n",
    "                                   activation='relu', name='conv_2'),\n",
    "            tf.keras.layers.BatchNormalization(name='bn_2'),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(1, 2), name='maxpool_2')(x),\n",
    "            # 75x75x64\n",
    "            \n",
    "            tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=(2, 2), padding='same',\n",
    "                                   activation='relu', name='conv_3'),\n",
    "            # 38x38x64\n",
    "            tf.keras.layers.BatchNormalization(name='bn_3'),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(1, 2), name='maxpool_3')(x),\n",
    "            # 19x19x64\n",
    "              \n",
    "            tf.keras.layers.Flatten(),\n",
    "            # No activation\n",
    "            tf.keras.layers.Dense(latent_dim + latent_dim),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    self.decoder = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
    "            tf.keras.layers.Dense(units=19*19*64, activation='relu'),\n",
    "            tf.keras.layers.Reshape(target_shape=(19,19,64)),\n",
    "            tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same',\n",
    "                                            activation='relu'),\n",
    "            tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, strides=2, padding='same',\n",
    "                                            activation='relu'),\n",
    "            # No activation\n",
    "            tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, strides=1, padding='same'),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "  @tf.function\n",
    "  def sample(self, eps=None):\n",
    "    if eps is None:\n",
    "      eps = tf.random.normal(shape=(100, self.latent_dim))\n",
    "    return self.decode(eps, apply_sigmoid=True)\n",
    "\n",
    "  def encode(self, x):\n",
    "    mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "    return mean, logvar\n",
    "\n",
    "  def reparameterize(self, mean, logvar):\n",
    "    eps = tf.random.normal(shape=mean.shape)\n",
    "    return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "  def decode(self, z, apply_sigmoid=False):\n",
    "    logits = self.decoder(z)\n",
    "    if apply_sigmoid:\n",
    "      probs = tf.sigmoid(logits)\n",
    "      return probs\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T08:28:27.591922Z",
     "start_time": "2020-10-29T08:28:27.514583Z"
    }
   },
   "outputs": [],
   "source": [
    "# def encoder(input_encoder):\n",
    "\n",
    "inputs = keras.Input(shape=[75,300,1], name='input_layer')\n",
    "#75x300x1\n",
    "\n",
    "# Conv + MaxPooling\n",
    "x = keras.layers.Conv2D(32, kernel_size=3, strides= 1, padding='same', name='conv_1')(inputs)\n",
    "x = keras.layers.BatchNormalization(name='bn_1')(x)\n",
    "x = keras.layers.ReLU(name='relu_1')(x)\n",
    "x = keras.layers.MaxPooling2D(pool_size=(1, 2), name='maxpool_1')(x) \n",
    "#75x150x32\n",
    "\n",
    "# Conv + MaxPooling\n",
    "x = keras.layers.Conv2D(64, kernel_size=3, strides=1, padding='same', name='conv_2')(x)  # 卷积步幅设置为1\n",
    "x = keras.layers.BatchNormalization(name='bn_2')(x)\n",
    "x = keras.layers.ReLU(name='relu_2')(x)\n",
    "x = keras.layers.MaxPooling2D(pool_size=(1, 2), name='maxpool_2')(x) \n",
    "#75x75x64\n",
    "\n",
    "# Conv + MaxPooling\n",
    "x = keras.layers.Conv2D(64, kernel_size=3, strides=2, padding='same', name='conv_3')(x)  # 卷积步幅设置为1\n",
    "#38x38x64\n",
    "x = keras.layers.BatchNormalization(name='bn_3')(x)\n",
    "x = keras.layers.ReLU(name='relu_3')(x)\n",
    "x = keras.layers.MaxPooling2D(pool_size=(2, 2), name='maxpool_3')(x) \n",
    "#19x19x64\n",
    "    \n",
    "# Flatten\n",
    "flatten = keras.layers.Flatten(name='flatten_1')(x)\n",
    "bottleneck = keras.layers.Dense(16, name='dense_1')(flatten)\n",
    "\n",
    "model = tf.keras.Model(inputs, bottleneck, name=\"Encoder\")\n",
    "model.summary()\n",
    "    # return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(input_decoder):\n",
    "    # Input is the bottleneck vector\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
